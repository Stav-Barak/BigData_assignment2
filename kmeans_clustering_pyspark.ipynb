{"cells":[{"cell_type":"code","source":["from pyspark import *\nimport random\nimport pandas as pd\nimport statistics as st\nfrom pyspark.sql import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import calinski_harabasz_score, adjusted_rand_score"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"17081a1d-ec0f-4e94-836b-948209df1679","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def kmeans_algorithm(data, k, convergence_threshold=0.0001, num_iterations=30):\n    # Convert DataFrame to Pandas DataFrame and apply MinMaxScaler\n    pandas_df = MinMaxScaler().fit_transform(data.toPandas())\n    pandas_df = pd.DataFrame(pandas_df, columns=data.columns)\n\n    # Create Spark DataFrame from Pandas DataFrame\n    df = spark.createDataFrame(pandas_df)\n\n    # Convert df to rdd\n    data_rdd = df.rdd\n    \n    # Change data types with map\n    data_rdd = data_rdd.map(lambda row: [float(x) for x in row])\n    \n    # Step 1: Randomly pick K data points from the data as initial centroids\n    initial_centroids = data_rdd.takeSample(False, k)\n    \n    # Distance between two points\n    euclidean_distance = lambda p, center: (next(i for i, c in enumerate(initial_centroids) if c == center), sum((p[i] - center[i])**2 for i in range(len(p)-1))**0.5)\n\n    check = lambda observation, j: 1 if observation[-1] == j else 0\n\n    for i in range(num_iterations):\n        is_converged = 1\n        # Step 2 b: Assign data points to the nearest centroid\n        assign_cluster = lambda p: [item for item in p] + [int(min((euclidean_distance(p,center) for center in initial_centroids), key = lambda x: x[1])[0])]\n        classification = data_rdd.map(assign_cluster)\n\n\n        # Step 3: Calculate new centroids\n        sum_points = lambda p1, p2: [p1[i] + p2[i] for i in range(len(p1))]\n        for j in range(k):\n            cluster_j = classification.filter(lambda obs: check(obs, j))\n            sum_cluster = cluster_j.reduce(sum_points)\n            last_center_at_j = initial_centroids[j]\n            initial_centroids[j] = [x / cluster_j.count() for x in sum_cluster]\n\n\n            if euclidean_distance(last_center_at_j, initial_centroids[j])[1] >= convergence_threshold:\n                is_converged = 0\n            # STOP because is converged \n            else:\n                is_converged = 1\n                break\n                \n\n        # Stop\n        if is_converged == 1:\n            break\n\n    return classification\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fba21065-4ff7-4b8b-8a44-884b18d12adb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def check_each_data(data, k_values, exp):\n    for k in k_values:\n        print(f\"K = {k}\")\n        ch_scores = []  # Store CH scores for each experiment\n        ari_scores = []  # Store ARI scores for each experiment\n        for r in range(exp):  # Run 'exp' experiments\n            classification = kmeans_algorithm(data, k)\n            ch_true = classification.map(lambda row: row[:-2]).collect()\n            ch_pred = classification.map(lambda row: row[-1]).collect()\n            ari_true = data.select(\"Class\").toPandas()[\"Class\"].tolist()\n            ari_pred = classification.map(lambda row: row[-1]).collect()\n\n            # Calculate CH and ARI scores\n            ch_score = calinski_harabasz_score(ch_true, ch_pred)\n            ari_score = adjusted_rand_score(ari_true, ari_pred)\n\n            ch_scores.append(ch_score)\n            ari_scores.append(ari_score)\n\n        avg_ch_score = sum(ch_scores) / len(ch_scores)\n        std_ch_score = st.stdev(ch_scores)\n        avg_ari_score = sum(ari_scores) / len(ari_scores)\n        std_ari_score = st.stdev(ari_scores)\n        print(f\"The average and standard deviation values of the CH measure across all {exp} experiments: ({avg_ch_score:.2f} ; {std_ch_score:.2f})\")\n        print(f\"The average and standard deviation values of the ARI measure across all {exp} experiments: ({avg_ari_score:.2f} ; {std_ari_score:.2f})\")\n        print(\"\")\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"036ba10e-bd5e-4be9-a55f-04b5efe594ab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dataset_path = \"/FileStore/tables/iris.csv\"\n\n# values of K\nk_values = [2, 3, 4, 5, 6]\n\nexp = 10\nprint (\"iris\")\nfile_type = \"csv\"\ndata = spark.read.csv(dataset_path, header=True)\ncheck_each_data(data, k_values, exp)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1fbb5949-abe5-479f-be4c-3a8e23c93fba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["iris\nK = 2\nThe average and standard deviation values of the CH measure across all 10 experiments: (353.37 ; 0.00)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.57 ; 0.00)\n\nK = 3\nThe average and standard deviation values of the CH measure across all 10 experiments: (302.55 ; 71.51)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.62 ; 0.14)\n\nK = 4\nThe average and standard deviation values of the CH measure across all 10 experiments: (260.41 ; 71.50)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.60 ; 0.11)\n\nK = 5\nThe average and standard deviation values of the CH measure across all 10 experiments: (241.13 ; 42.56)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.56 ; 0.10)\n\nK = 6\nThe average and standard deviation values of the CH measure across all 10 experiments: (212.57 ; 43.55)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.54 ; 0.07)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndataset_path = \"/FileStore/tables/glass.csv\"\n\n# values of K\nk_values = [2, 3, 4, 5, 6]\n\nexp = 10\nprint (\"glass\")\nfile_type = \"csv\"\ndata = spark.read.csv(dataset_path, header=True)\ncheck_each_data(data, k_values, exp)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4972659c-bd20-457a-9599-5785bfdc6d2e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["glass\nK = 2\nThe average and standard deviation values of the CH measure across all 10 experiments: (121.71 ; 47.55)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.16 ; 0.10)\n\nK = 3\nThe average and standard deviation values of the CH measure across all 10 experiments: (101.10 ; 4.59)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.15 ; 0.04)\n\nK = 4\nThe average and standard deviation values of the CH measure across all 10 experiments: (84.48 ; 11.19)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.16 ; 0.04)\n\nK = 5\nThe average and standard deviation values of the CH measure across all 10 experiments: (71.45 ; 11.68)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.14 ; 0.03)\n\nK = 6\nThe average and standard deviation values of the CH measure across all 10 experiments: (65.91 ; 9.94)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.15 ; 0.03)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndataset_path = \"/FileStore/tables/parkinsons.csv\"\n\n# values of K\nk_values = [2, 3, 4, 5, 6]\n\nexp = 10\nprint (\"parkinsons\")\nfile_type = \"csv\"\ndata = spark.read.csv(dataset_path, header=True)\ncheck_each_data(data, k_values, exp)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"432af3dd-94f8-40e6-9b62-b391f4f37be4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["parkinsons\nK = 2\nThe average and standard deviation values of the CH measure across all 10 experiments: (84.22 ; 0.00)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.05 ; 0.00)\n\nK = 3\nThe average and standard deviation values of the CH measure across all 10 experiments: (75.74 ; 1.38)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.07 ; 0.02)\n\nK = 4\nThe average and standard deviation values of the CH measure across all 10 experiments: (60.87 ; 7.28)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.08 ; 0.04)\n\nK = 5\nThe average and standard deviation values of the CH measure across all 10 experiments: (58.04 ; 6.30)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.07 ; 0.05)\n\nK = 6\nThe average and standard deviation values of the CH measure across all 10 experiments: (55.54 ; 2.65)\nThe average and standard deviation values of the ARI measure across all 10 experiments: (0.09 ; 0.04)\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-15 22:49:50","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
